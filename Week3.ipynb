{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6057968f",
   "metadata": {},
   "source": [
    "Why are vectors needed?\n",
    "Vectors are needed so that words can be converted to numbers that computer understands\n",
    "\n",
    "\n",
    "B-o-W = Bag of Words\n",
    "\n",
    "TF-IDF = Term Frequency - Inverse Document Frequency\n",
    "\n",
    "\n",
    "vb. TF\n",
    "\n",
    "\"I love Python\"  => doc1\n",
    "\"I love coding for life\" => doc2\n",
    "\"I hate bugs\" => doc3\n",
    "\n",
    "\n",
    "            Doc1    Doc2    Doc3\n",
    "I           1       1       1\n",
    "love        1       1       0\n",
    "coding       ..  \n",
    "life    ...\n",
    "learning       ...\n",
    "hate    ...\n",
    "bugs    ....\n",
    "\n",
    "            3       5       3\n",
    "            \n",
    "\n",
    "\n",
    "word \"I\"  is IDF = log(3/3) = 0\n",
    "word \"python\" is IDF = log(3/1) = 1.1\n",
    "\n",
    "word \"love\" is IDF = log(3/2) = 0.18\n",
    "\n",
    "\n",
    "\n",
    "TF-IDF = TF x IDF  \n",
    "\n",
    "\n",
    "\n",
    "            Doc1    Doc2    Doc3\n",
    "I           o       0       0\n",
    "love        0.18     0.18       0\n",
    "coding       ..  \n",
    "life    ...\n",
    "learning       ...\n",
    "hate    ...\n",
    "bugs    ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e8b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c7a9d",
   "metadata": {},
   "source": [
    "Install scikit-learn with the following code:\n",
    "pip install scikit-learn\n",
    "\n",
    "This is a Python library for Machine Learning.\n",
    "It provides ready-to-use tools to:\n",
    "\n",
    "-Train machine learning models\n",
    "-Process and transform data\n",
    "-Evaluate performance\n",
    "-Do tasks like classification, clustering, and text vectorization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a49fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bugs' 'coding' 'hate' 'love' 'python']\n",
      "[[0 0 0 1 1]\n",
      " [0 1 0 1 0]\n",
      " [1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of Bag of Words (BoW)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love Python.\",\n",
    "    \"I love coding\",\n",
    "    \"I hate bugs\"\n",
    "]\n",
    "\n",
    "# Create the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the feature names\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert the result to an array and print it\n",
    "print(X.toarray())\n",
    "\n",
    "#it doesnt take words like I, the, and, is into account. These are called stop words and are usually filtered out in text processing tasks.\n",
    "\n",
    "# e.g. I eat chicken = chicken eat I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8cd51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bugs' 'coding' 'hate' 'love' 'python']\n",
      "[[0.         0.         0.         0.60534851 0.79596054]\n",
      " [0.         0.79596054 0.         0.60534851 0.        ]\n",
      " [0.70710678 0.         0.70710678 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love Python.\",\n",
    "    \"I love coding.\",\n",
    "    \"I hate bugs\"\n",
    "]\n",
    "\n",
    "\n",
    "# Create the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the feature names\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# Convert the result to an array and print it\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9c48f",
   "metadata": {},
   "source": [
    "Vector transformation\n",
    "\n",
    "- Normalisatie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00782b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.6        0.8       ]\n",
      " [0.10482848 0.31448545 0.94345635]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[0, 3, 4],\n",
    "              [1, 3, 9]])\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "X_normalized = normalizer.fit_transform(x)\n",
    "\n",
    "print(X_normalized)\n",
    "\n",
    "#This is to normilize the doc vectors so that they have a length of 1.\n",
    "# This is often done to ensure that the vectors are comparable in terms of direction rather than magnitude,\n",
    "# which can be important in various machine learning and information retrieval tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64d2dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print ( 0**2 + 0.6**2 + 0.8**2 ) # = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff0d3090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['bugs' 'coding' 'hate' 'love' 'python']\n",
      "\n",
      "TF-IDF matrix (zonder normalisatie):\n",
      "[[0.         0.         0.         0.60534851 0.79596054]\n",
      " [0.         0.79596054 0.         0.60534851 0.        ]\n",
      " [0.70710678 0.         0.70710678 0.         0.        ]]\n",
      "\n",
      "Na normalisatie:\n",
      "[[0.         0.         0.         0.60534851 0.79596054]\n",
      " [0.         0.79596054 0.         0.60534851 0.        ]\n",
      " [0.70710678 0.         0.70710678 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Voorbeeldteksten\n",
    "texts = [\n",
    "    \"I love Python\",\n",
    "    \"I love coding\",\n",
    "    \"I hate bugs\"\n",
    "]\n",
    "\n",
    "# Stap 1: TF-IDF vectoren maken\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF matrix (zonder normalisatie):\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Stap 2: Normaliseren\n",
    "normalizer = Normalizer()\n",
    "X_normalized = normalizer.fit_transform(X.toarray())\n",
    "\n",
    "print(\"\\nNa normalisatie:\")\n",
    "print(X_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine similarity = (A . B) / (||A|| ||B||)\n",
    "\n",
    "# A . B is the dot product of A and B\n",
    "# ||A|| is the magnitude (length) of vector A\n",
    "# ||B|| is the magnitude (length) of vector B\n",
    "# Cosine similarity measures the cosine of the angle between two non-zero vectors in an inner product space.\n",
    "# It is a measure of similarity between two vectors that considers their direction rather than their magnitude.\n",
    "# The cosine similarity is particularly useful in high-dimensional positive spaces,\n",
    "# such as text analysis, where the magnitude of the vectors may be less informative than their orientation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f83dc253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['above' 'and' 'bugs' 'learning' 'left' 'love' 'machine' 'nlp' 'right']\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "          Doc1      Doc2      Doc3\n",
      "Doc1  1.000000  0.857601  0.226682\n",
      "Doc2  0.857601  1.000000  0.226682\n",
      "Doc3  0.226682  0.226682  1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Voorbeeldteksten\n",
    "texts = [\n",
    "    \"I love left and above. I love NLP and machine learning\",\n",
    "    \"I love right and above. I love NLP and machine learning\",\n",
    "    \"I love bugs\"\n",
    "]   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Stap 1: TF-IDF vectoren maken\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "#words matrix\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(\"Feature names:\", words)\n",
    "\n",
    "cos_sim = cosine_similarity(X)\n",
    "df = pd.DataFrame(cos_sim, columns=[f'Doc{i+1}' for i in range(len(texts))], index=[f'Doc{i+1}' for i in range(len(texts))])\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d19280",
   "metadata": {},
   "source": [
    "Applying n-grams and other preprocessing techniques\n",
    "\n",
    "\n",
    "What are n-grams?\n",
    "Sequence of n consecutive words from a text\n",
    "\n",
    "\n",
    "e.g\n",
    "\n",
    "\"I love vs code\"\n",
    "\n",
    "\n",
    "n = 1 , unigram, [I, love, vs, code]\n",
    "\n",
    "n = 2, bigram,   [I love, Love vs, vs code]\n",
    "\n",
    "n = 3, trigram,  [I love vs,  love vs code]\n",
    "\n",
    "\n",
    "Why is this usefull?\n",
    "Because the meaning of something is very clear after bringing the words together\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"not good\"\n",
    "\n",
    "unigram = not , good\n",
    "bigram = not good\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d193fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams Feature names: ['deep' 'fun' 'is' 'learning' 'love' 'machine']\n",
      "Unigrams Matrix:\n",
      " [[0 0 0 1 1 1]\n",
      " [0 1 1 1 0 1]\n",
      " [1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\n",
    "    \"I love machine learning\",\n",
    "    \"machine learning is fun\",\n",
    "    \"I love deep learning\"\n",
    "]\n",
    "\n",
    "#Unigrams (1 word per feature)\n",
    "vectorizer_uni = CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "X_uni = vectorizer_uni.fit_transform(texts)\n",
    "print(\"Unigrams Feature names:\", vectorizer_uni.get_feature_names_out())\n",
    "print(\"Unigrams Matrix:\\n\", X_uni.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "371a8b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams Feature names: ['deep learning' 'is fun' 'learning is' 'love deep' 'love machine'\n",
      " 'machine learning']\n",
      "Bigrams Matrix:\n",
      " [[0 0 0 0 1 1]\n",
      " [0 1 1 0 0 1]\n",
      " [1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#bi-grams (2 words per feature)\n",
    "vectorizer_bi = CountVectorizer(ngram_range=(2,2))\n",
    "X_bi = vectorizer_bi.fit_transform(texts)\n",
    "print(\"\\nBigrams Feature names:\", vectorizer_bi.get_feature_names_out())\n",
    "print(\"Bigrams Matrix:\\n\", X_bi.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e189f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams Feature names: ['deep' 'deep learning' 'fun' 'is' 'is fun' 'learning' 'learning is'\n",
      " 'love' 'love deep' 'love machine' 'machine' 'machine learning']\n",
      "Trigrams Matrix:\n",
      " [[0 0 0 0 0 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 1 1 1 0 0 0 1 1]\n",
      " [1 1 0 0 0 1 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#tri-grams (3 words per feature)\n",
    "vectorizer_tri = CountVectorizer(ngram_range=(1,2))\n",
    "X_tri = vectorizer_tri.fit_transform(texts)\n",
    "print(\"\\nBigrams Feature names:\", vectorizer_tri.get_feature_names_out())\n",
    "print(\"Trigrams Matrix:\\n\", X_tri.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a67b56",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "Voordat we tekst vectoriseren, moeten we die opschonen\n",
    "\n",
    "Lowercasing\n",
    "tekens weghalen vb. kommas, punten, !\n",
    "\n",
    "stopwoorden weglaten\n",
    "\n",
    "lemmatization ->  woord naar basisvorm brengen  \"running\" -. \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e3f897b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re #regular expressions\n",
    "#regular expressions are expressions that define search patterns for strings.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20530df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I am LOVING machine-learning!!\",\n",
    "    \"Machine learning is amazing, and I love it.\",\n",
    "    \"I loved deep learning in Python.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cded5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loving machinelearning\n",
      "\n",
      "Bigrams Feature names: ['am loving' 'amazing and' 'and love' 'deep learning' 'in python'\n",
      " 'is amazing' 'learning in' 'learning is' 'love it' 'loved deep'\n",
      " 'loving machine' 'machine learning']\n",
      "Bigrams Matrix:\n",
      " [[1 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 1 1 0 0 1 0 1 1 0 0 1]\n",
      " [0 0 0 1 1 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# step 1 cleaning text\n",
    "def preprocess(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "print(preprocess(texts[0]))\n",
    "\n",
    "\n",
    "#step 2 verctorization with bi-grams and tri-grams\n",
    "\n",
    "vectorizer_bi = CountVectorizer(ngram_range=(2,2))\n",
    "X_bi = vectorizer_bi.fit_transform(texts)\n",
    "print(\"\\nBigrams Feature names:\", vectorizer_bi.get_feature_names_out())\n",
    "print(\"Bigrams Matrix:\\n\", X_bi.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854b3e9",
   "metadata": {},
   "source": [
    "preprocessing, TF-IDF, n-grams en cosine similarity.\n",
    "\n",
    "Hier is een complete demo die je zo in je college kunt laten zie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "995d0723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned texts:\n",
      " ['loving machinelearning', 'machine learning amazing love', 'loved deep learning python'] \n",
      "\n",
      "TF-IDF Features (unigrams + bigrams):\n",
      "['amazing' 'amazing love' 'deep' 'deep learning' 'learning'\n",
      " 'learning amazing' 'learning python' 'love' 'loved' 'loved deep' 'loving'\n",
      " 'loving machinelearning' 'machine' 'machine learning' 'machinelearning'\n",
      " 'python'] \n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.         0.         0.57735027 0.        ]\n",
      " [0.38988801 0.38988801 0.         0.         0.29651988 0.38988801\n",
      "  0.         0.38988801 0.         0.         0.         0.\n",
      "  0.38988801 0.38988801 0.         0.        ]\n",
      " [0.         0.         0.38988801 0.38988801 0.29651988 0.\n",
      "  0.38988801 0.         0.38988801 0.38988801 0.         0.\n",
      "  0.         0.         0.         0.38988801]] \n",
      "\n",
      "Cosine Similarity between documents:\n",
      "                               loving machinelearning  \\\n",
      "loving machinelearning                            1.0   \n",
      "machine learning amazing love                     0.0   \n",
      "loved deep learning python                        0.0   \n",
      "\n",
      "                               machine learning amazing love  \\\n",
      "loving machinelearning                                 0.000   \n",
      "machine learning amazing love                          1.000   \n",
      "loved deep learning python                             0.088   \n",
      "\n",
      "                               loved deep learning python  \n",
      "loving machinelearning                              0.000  \n",
      "machine learning amazing love                       0.088  \n",
      "loved deep learning python                          1.000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 0: Sample texts\n",
    "texts = [\n",
    "    \"I am LOVING machine-learning!!\",\n",
    "    \"Machine learning is amazing, and I love it.\",\n",
    "    \"I loved deep learning in Python.\"\n",
    "]\n",
    "\n",
    "# Step 1: Preprocessing function\n",
    "def preprocess(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to all texts\n",
    "cleaned_texts = [preprocess(t) for t in texts]\n",
    "print(\"Cleaned texts:\\n\", cleaned_texts, \"\\n\")\n",
    "\n",
    "# Step 2: TF-IDF with n-grams (unigrams + bigrams)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english'),  # remove stopwords again just to be sure\n",
    "    ngram_range=(1, 2)                      # unigrams + bigrams\n",
    ")\n",
    "X = vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "# Print features (words + n-grams)\n",
    "print(\"TF-IDF Features (unigrams + bigrams):\")\n",
    "print(vectorizer.get_feature_names_out(), \"\\n\")\n",
    "\n",
    "# Step 3: TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(X.toarray(), \"\\n\")\n",
    "\n",
    "# Step 4: Cosine similarity between documents\n",
    "cos_sim = cosine_similarity(X)\n",
    "df = pd.DataFrame(cos_sim, index=cleaned_texts, columns=cleaned_texts)\n",
    "print(\"Cosine Similarity between documents:\")\n",
    "print(df.round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
