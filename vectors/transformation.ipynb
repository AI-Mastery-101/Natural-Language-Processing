{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e10250e",
   "metadata": {},
   "source": [
    "Wat betekent “vector transformation”?\n",
    "\n",
    "Tot nu toe hebben we tekst omgezet naar vectoren (BoW, TF-IDF).\n",
    "Maar vaak moeten we deze vectoren transformeren, zodat ze:\n",
    "\n",
    "- geschikt worden voor machine learning modellen,\n",
    "- gestandaardiseerd of verkleind worden om beter te presteren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede9876",
   "metadata": {},
   "source": [
    "Wat kunnen we doen met vectoren?\n",
    " Normalisatie / Schalen\n",
    "\n",
    "We zorgen dat vectoren een vergelijkbare lengte hebben.\n",
    "Bij TF-IDF is dit vaak al genormaliseerd, maar bij BoW niet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a995e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization:\n",
      " [[0 1 1]\n",
      " [1 0 1]]\n",
      "\n",
      "After normalization:\n",
      " [[0.         0.70710678 0.70710678]\n",
      " [0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"I love NLP\", \"I hate NLP\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_vectors = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "normalized = norm.fit_transform(bow_vectors)\n",
    "\n",
    "print(\"Before normalization:\\n\", bow_vectors)\n",
    "print(\"\\nAfter normalization:\\n\", normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26441020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (woordenlijst): ['hate' 'love' 'nlp']\n",
      "\n",
      "Bag-of-Words matrix (voor normalisatie):\n",
      "[[0 1 1]\n",
      " [1 0 1]]\n",
      "\n",
      "Na normalisatie:\n",
      "[[0.         0.70710678 0.70710678]\n",
      " [0.70710678 0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ De originele teksten\n",
    "# ===============================\n",
    "texts = [\"I love NLP\", \"I hate NLP\"]\n",
    "\n",
    "# ===============================\n",
    "# 2️⃣ Maak de vectorizer aan\n",
    "# ===============================\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# ===============================\n",
    "# 3️⃣ Pas de vectorizer toe op de teksten\n",
    "#     (fit = leer het vocabulaire)\n",
    "#     (transform = maak de numerieke vectoren)\n",
    "# ===============================\n",
    "bow_vectors = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# ===============================\n",
    "# 4️⃣ Bekijk het vocabulaire\n",
    "# ===============================\n",
    "print(\"Vocabulary (woordenlijst):\", vectorizer.get_feature_names_out())\n",
    "# Output: ['hate' 'i' 'love' 'nlp']\n",
    "\n",
    "# ===============================\n",
    "# 5️⃣ Bekijk de Bag-of-Words matrix\n",
    "# ===============================\n",
    "print(\"\\nBag-of-Words matrix (voor normalisatie):\")\n",
    "print(bow_vectors)\n",
    "\n",
    "# De matrix ziet er zo uit:\n",
    "# [[0 1 1 1]   ← \"I love NLP\"\n",
    "#  [1 1 0 1]]  ← \"I hate NLP\"\n",
    "\n",
    "# Betekenis per kolom:\n",
    "#   hate | i | love | nlp\n",
    "#   0      1    1      1   → \"I love NLP\"\n",
    "#   1      1    0      1   → \"I hate NLP\"\n",
    "#\n",
    "# 1 betekent: woord komt voor in de zin\n",
    "# 0 betekent: woord komt NIET voor\n",
    "\n",
    "# ===============================\n",
    "# 6️⃣ Normaliseren van vectoren\n",
    "# ===============================\n",
    "norm = Normalizer()\n",
    "normalized = norm.fit_transform(bow_vectors)\n",
    "\n",
    "print(\"\\nNa normalisatie:\")\n",
    "print(normalized)\n",
    "\n",
    "# Normalisatie zorgt dat elke rij (documentvector)\n",
    "# dezelfde lengte krijgt.\n",
    "# Het verandert de schaal (de grootte van de getallen)\n",
    "# maar niet de verhoudingen tussen woorden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad77a4",
   "metadata": {},
   "source": [
    "Dimensionality Reduction (optioneel)\n",
    "\n",
    "Soms heb je duizenden woorden, dus ook duizenden features.\n",
    "Dat is zwaar voor het model → we kunnen de dimensie verkleinen.\n",
    "\n",
    "Bijv. met TruncatedSVD (Latent Semantic Analysis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1638e441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (3, 10)\n",
      "Reduced shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"Machine learning is fun\",\n",
    "    \"Deep learning drives AI\",\n",
    "    \"I love studying AI and machine learning\"\n",
    "]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(docs)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape:\", X_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05863ad",
   "metadata": {},
   "source": [
    "Wat betekent “Dimensionality Reduction”?\n",
    "\n",
    "Wanneer je tekst omzet in cijfers (zoals met Bag-of-Words of TF-IDF), krijg je vaak duizenden kolommen —\n",
    "elke kolom is een woord in je dataset.\n",
    "\n",
    "\n",
    "| Tekst                        | love | hate | NLP | AI | deep | learning | model | accuracy |\n",
    "| ---------------------------- | ---- | ---- | --- | -- | ---- | -------- | ----- | -------- |\n",
    "| \"I love NLP\"                 | 1    | 0    | 1   | 0  | 0    | 0        | 0     | 0        |\n",
    "| \"I hate NLP\"                 | 0    | 1    | 1   | 0  | 0    | 0        | 0     | 0        |\n",
    "| \"AI model improves accuracy\" | 0    | 0    | 0   | 1  | 0    | 0        | 1     | 1        |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hier heb je 8 kolommen (dimensies) —\n",
    "maar in een echte dataset kunnen dat 10.000 woorden zijn → 10.000 dimensies!\n",
    "\n",
    "Dat is:\n",
    "zwaar voor je computer\n",
    "moeilijk voor modellen om mee te werken\n",
    "en veel woorden lijken op elkaar (bijv. love, like, enjoy)\n",
    "\n",
    "Dus we willen die informatie samenvatten → met minder dimensies, maar zonder te veel betekenis te verliezen.\n",
    "Dat proces heet dimensionality reduction.\n",
    "\n",
    "\n",
    "\n",
    "Je kunt het vergelijken met samenvatten van een lange tekst:\n",
    "In plaats van 10.000 woorden te onthouden, wil je 2 of 3 belangrijke thema’s vinden.\n",
    "Bijvoorbeeld: “positieve emotie”, “negatieve emotie”, “AI-technisch”.\n",
    "Dat doen we wiskundig met methodes zoals:\n",
    "TruncatedSVD (Latent Semantic Analysis – LSA)\n",
    "PCA (Principal Component Analysis)\n",
    "UMAP / t-SNE (voor visualisaties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b999dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aantal woorden (dimensies): 17\n",
      "\n",
      "Gereduceerde representatie:\n",
      "        Thema 1       Thema 2\n",
      "0  7.654210e-01 -2.767374e-01\n",
      "1  2.667281e-01  8.337300e-01\n",
      "2  6.355989e-01 -4.234057e-01\n",
      "3 -4.281756e-16 -1.788692e-15\n",
      "4  6.849050e-01  3.775087e-01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Stel we hebben meerdere documenten\n",
    "texts = [\n",
    "    \"I love NLP and machine learning\",\n",
    "    \"I hate spam emails\",\n",
    "    \"Deep learning improves NLP\",\n",
    "    \"AI models learn from data\",\n",
    "    \"Spam detection with machine learning\"\n",
    "]\n",
    "\n",
    "# 2️⃣ Zet ze om naar TF-IDF vectoren\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(\"Aantal woorden (dimensies):\", X.shape[1])\n",
    "\n",
    "# 3️⃣ Verminder de dimensies met TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)  # we houden maar 2 'thema’s' over\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "# 4️⃣ Bekijk het resultaat\n",
    "df = pd.DataFrame(X_reduced, columns=[\"Thema 1\", \"Thema 2\"])\n",
    "print(\"\\nGereduceerde representatie:\")\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
